---
title: "Simulation Study 1 Results"
---

## Results from Study 1 {#results1-intro}

In this section, the results from the first simulation study are reported. The development simulation parameters were consistent with the validation parameters except there was no missingness. Sample size was varied at n=500, n=10,000 and n=100,000 and outcome prevalance varied at 1%, 5% and 10%. The missingess in the validation dataset was varied at 25%, 50% and 75% of one predictor variable x1. The first section describes the scenario where n=500. 


```{r}
#| label: load_packages 
#| include: false
library(dplyr)
library(ggplot2)
library(tidyr)
```


```{r}
#| label: open_datasets_n500 
#| include: false
## Set working directory 
setwd("C:\\Users\\maecj\\OneDrive - Nexus365\\A DPhil\\Simulation studies\\Programs\\Study 1\\SimulationStudy1_11Jun2024\\SimulationStudy\\Data") 

## Load required datasets 
########################

load("Results_1_Nval_500_Yprev_0.01_Rprev_0.25_22Aug2024.Rdata")
  simresults_Yprev1Rprev25 <- simulation_results
load("Results_2_Nval_500_Yprev_0.01_Rprev_0.5_22Aug2024.Rdata")
  simresults_Yprev1Rprev50 <- simulation_results
load("Results_3_Nval_500_Yprev_0.01_Rprev_0.75_22Aug2024.Rdata")
  simresults_Yprev1Rprev75 <- simulation_results
load("Results_4_Nval_500_Yprev_0.05_Rprev_0.25_21Aug2024.Rdata")
  simresults_Yprev5Rprev25 <- simulation_results
load("Results_5_Nval_500_Yprev_0.05_Rprev_0.5_21Aug2024.Rdata")
  simresults_Yprev5Rprev50 <- simulation_results
load("Results_6_Nval_500_Yprev_0.05_Rprev_0.75_21Aug2024.Rdata")
  simresults_Yprev5Rprev75 <- simulation_results
load("Results_7_Nval_500_Yprev_0.1_Rprev_0.25_21Aug2024.Rdata")
  simresults_Yprev10Rprev25 <- simulation_results
load("Results_8_Nval_500_Yprev_0.1_Rprev_0.5_21Aug2024.Rdata")
  simresults_Yprev10Rprev50 <- simulation_results
load("Results_9_Nval_500_Yprev_0.1_Rprev_0.75_21Aug2024.Rdata")
  simresults_Yprev10Rprev75 <- simulation_results

```

```{r}
#| label: create_plotabledfs 
#| include: false


# Extract target measures 
################################
      extract_measures_fnc <- function(simresults) {
        combined_data <- list()
      
      for (i in 1:length(simresults[["iterations"]])) {
        df <- simresults[["iterations"]][[i]][["preds"]][["target_measures"]]
        df <- cbind(iteration = i, df)  # Add the iteration column
        combined_data[[i]] <- df
      }
      
      return(do.call(rbind, combined_data))
    }
  
  # Combine data for both datasets
  target_measures <- list(
    simresults_Yprev10Rprev75 = extract_measures_fnc(simresults_Yprev10Rprev75),
    simresults_Yprev10Rprev50 = extract_measures_fnc(simresults_Yprev10Rprev50),
    simresults_Yprev10Rprev25 = extract_measures_fnc(simresults_Yprev10Rprev25),
    simresults_Yprev5Rprev75 = extract_measures_fnc(simresults_Yprev5Rprev75),
    simresults_Yprev5Rprev50 = extract_measures_fnc(simresults_Yprev5Rprev50),
    simresults_Yprev5Rprev25 = extract_measures_fnc(simresults_Yprev5Rprev25),
    simresults_Yprev1Rprev75 = extract_measures_fnc(simresults_Yprev1Rprev75),
    simresults_Yprev1Rprev50 = extract_measures_fnc(simresults_Yprev1Rprev50),
    simresults_Yprev1Rprev25 = extract_measures_fnc(simresults_Yprev1Rprev25)
  )
  

################################################################################
## Store n, bias 
################################################################################
## Bias here is how far away the predicted values of the outcome are from the true value

## The Root Mean Square Error RMSE
######################################
## RMSE aggregates error due to bias and variability 
## 0 = perfect imputation
## >0 = decreasing performance of the imputation (clinical relevance depends on range of predictor)
## Formula: 

## Calibration in the Large
######################################
# Calibration-in-the-large measures whether the overall predicted risk matches the observed risk. 
# Essentially, it checks if the average predicted probability is equal to the average observed outcome.
# For example, if a model predicts an average risk of 10% for a certain event, 
# calibration-in-the-large would be satisfied if the actual event rate is also 10%.



## Create summary of bias 
#####################################
# Define the number of iterations
num_iterations <- 100

  
  # Initialize empty lists to store bias summaries for each dataset
  bias_summaries <- list(
    simresults_Yprev10Rprev25 = data.frame(),
    simresults_Yprev10Rprev50 = data.frame(),
    simresults_Yprev10Rprev75 = data.frame(),
    simresults_Yprev5Rprev75 = data.frame(),
    simresults_Yprev5Rprev50 = data.frame(),
    simresults_Yprev5Rprev25 = data.frame(),
    simresults_Yprev1Rprev75 = data.frame(),
    simresults_Yprev1Rprev50 = data.frame(),
    simresults_Yprev1Rprev25 = data.frame()
  )

# Go through each dataset and iteration to calculate
for (i in 1:num_iterations) {
  # Loop through each dataset
  for (dataset_name in names(bias_summaries)) {
    dataset <- get(dataset_name)
  
  # Loop through each method
  for (method_id in c("CCA_val_data", "mean_val", "MI_val_data_noY", "MI_val_data_withY")) {
  
    # Extract true Y and estimated Y values
    true_Y <- dataset[["iterations"]][[i]][["preds"]][["preds_per_data_set"]][[method_id]][["Y"]]
    estimated_Y <- dataset[["iterations"]][[i]][["preds"]][["preds_per_data_set"]][[method_id]][["Prediction_Model"]]
    
    # Calculate number of observations (n)
    n <- length(true_Y)
    
    # Calculate bias
    bias <- sum(true_Y - estimated_Y) / n
    
    # Calculate MSE 
    mse <- sum((true_Y - estimated_Y)^2) / n
    
    # Calculate RMSE 
   rmse <- sqrt(mse)
    
    # Create a temporary row for the current iteration and dataset
    new_row <- data.frame(iteration = i, dataset = method_id, n = n, bias = bias, mse=mse, rmse=rmse)
    
    # Append the temporary row to the corresponding bias_summary data frame
    bias_summaries[[dataset_name]] <- rbind(bias_summaries[[dataset_name]], new_row)
  }
}
}
  
  
################################################################################
# Combine target measures and bias_summary
#################################################################################
  combined_summaries <- list()
  
  for (dataset_name in names(target_measures)) {
    target_measures_df <- as.data.frame(target_measures[[dataset_name]])
    bias_summary_df <- as.data.frame(bias_summaries[[dataset_name]])
    
    combined_df <- full_join(target_measures_df, bias_summary_df, by = c("iteration", "dataset"))
    
    combined_summaries[[dataset_name]] <- combined_df
  }
  

##############################################################################
## Calculate average across simulations 
##############################################################################
## Create summaries
 #############################################
  # Function to summarize the data
  summarize_data <- function(df) {
    df %>%
      group_by(dataset) %>%
      summarise(across(c(Cal_Int, Cal_Slope, AUC, Brier, bias, mse, rmse), 
                       list(AVG = ~ mean(.x, na.rm = TRUE),  
                            LCI = ~ quantile(.x, 0.025, na.rm = TRUE), 
                            UCI = ~ quantile(.x, 0.975, na.rm = TRUE), 
                            NACount = ~ sum(is.na(.x))), 
                       .names = "{fn}-{col}"))
  }
  
  # Apply the summarization function to each data frame in the combined_summaries list
  summarized_summaries <- lapply(combined_summaries, summarize_data)
  


## Reshape long 
  ################################################################
  # Combine the summarized data frames into a single data frame
  combined_summarized_df <- bind_rows(summarized_summaries, .id = "df")
  
  # Reshape the combined data frame to a long format
  simulation_parameters_long <- combined_summarized_df %>%
    pivot_longer(cols = -c(dataset, df),
                 names_to = c(".value", "Metric"), 
                 names_sep = "-")
  
  # Rename metric to measure
  simulation_parameters_long <- simulation_parameters_long %>%
    mutate(Measure = case_when(
      Metric == "Cal_Int" ~ "Calibration in the Large", 
      Metric == "Cal_Slope" ~ "Calibration Slope", 
      Metric == "AUC" ~ "AUC",
      Metric == "Brier" ~ "Brier Score", 
      Metric == "bias" ~ "Bias",
      Metric == "mse" ~ "Mean Square Error",
      Metric == "rmse" ~ "Root Mean Square Error"),
    Method = case_when(dataset == "CCA_val_data" ~"Complete Case Analysis", 
                       dataset == "mean_val" ~"Mean Imputation",
                       dataset == "MI_val_data_noY" ~ "Multiple Imputation without Outcome",
                       dataset == "MI_val_data_withY" ~  "Multiple Imputation with Outcome"),
    Parameter = case_when(
      df == "simresults_Yprev10Rprev25" ~ "Outcome prevalence 10% and Missingness 25%",
      df == "simresults_Yprev10Rprev50" ~ "Outcome prevalence 10% and Missingness 50%",
      df == "simresults_Yprev10Rprev75" ~ "Outcome prevalence 10% and Missingness 75%",
      df == "simresults_Yprev5Rprev25" ~ "Outcome prevalence 5% and Missingness 25%",
      df == "simresults_Yprev5Rprev50" ~ "Outcome prevalence 5% and Missingness 50%",
      df == "simresults_Yprev5Rprev75" ~ "Outcome prevalence 5% and Missingness 75%",
      df == "simresults_Yprev1Rprev25" ~ "Outcome prevalence 1% and Missingness 25%",
      df == "simresults_Yprev1Rprev50" ~ "Outcome prevalence 1% and Missingness 50%",
      df == "simresults_Yprev1Rprev75" ~ "Outcome prevalence 1% and Missingness 75%")) %>%
    mutate(Parameter = factor(Parameter, levels = c(
    "Outcome prevalence 1% and Missingness 25%",
    "Outcome prevalence 1% and Missingness 50%",
    "Outcome prevalence 1% and Missingness 75%",
    "Outcome prevalence 5% and Missingness 25%",
    "Outcome prevalence 5% and Missingness 50%",
    "Outcome prevalence 5% and Missingness 75%",
    "Outcome prevalence 10% and Missingness 25%",
    "Outcome prevalence 10% and Missingness 50%",
    "Outcome prevalence 10% and Missingness 75%"
  )))
  
  # Add Scale Group
  simulation_parameters_long$scale_group <- ifelse(
    simulation_parameters_long$Metric %in% c("CalSlope", "AUC"), 
    "Group1", 
    "Group2"
  )

```
### Predictive Performance from Study 1 N=500 under Missing at Random {#-redictive-performance-n500}

#### Brier Score 
The Brier score ranges between 0 (perfect accuracy) and 1 (perfect inaccuracy). There was no difference in performance of the imputation methods in any scenario. The lowest brier scores occurred at lower outcome prevalence.  
```{r #fig-brier}
#| label: brier_score
#| echo: false
#| fig-cap: Brier Score for each combination of prevalence, sample size, missingness under Missing at Random
#| fig-width: 10
#| fig-height: 12 

ggplot(simulation_parameters_long %>% filter(Measure=="Brier Score"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.2) +
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
    # scale_x_continuous(limits = c(0.042, 0.092), breaks = seq(0.04, 0.1, by = 0.004)) +    
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick
  
```

#### Discrimination
The discrimination was calculated as the Area Under the Curve (AUC). Higher scores indicate better discrimination with 0.5 indicating the model is no better than chance. 

```{r #fig-auc} 
#| label: auc
#| echo: false
#| fig-cap: AUC for each combination of prevalence, sample size, missingness under Missing at Random
#| fig-width: 10
#| fig-height: 12 

ggplot(simulation_parameters_long %>% filter(Measure=="AUC"), 
         aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.2) +
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
  #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
  #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
 #   xlim(0.72,0.79)+
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick

```

#### Calibration in the Large and Calibration Slope
The Calibration was assessed through Calibration in the Large (CATL) and the Calibration Slope. 

The ideal value of CATL is 0, which indicates perfect calibration, positive values indicate the model is underestimating the risk while negative values indicate overestimation. Larger deviations from 0 suggest poorer calibration. 



```{r #fig-calitl} 
#| label: calitl
#| echo: false
#| fig-cap: Calibration in the Large for each combination of prevalence, sample size, missingness under Missing at Random
#| fig-width: 10
#| fig-height: 12 

    ggplot(simulation_parameters_long %>% filter(Measure=="Calibration in the Large"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.2) +
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick
  
  
```

The ideal value of the Calibration Slope is 1 indicating perfect calibration across all risk levels. Values less than 1 suggest overfitting (predictions are too extreme), while values greater than 1 suggest underfitting (predictions are too conservative). Values that differ significantly from 1 indicate poor calibration. 

``` {r #fig-calslope}
#| label: calslope
#| echo: false
#| fig-cap: Calibration Slope for each combination of prevalence, sample size, missingness under Missing at Random
#| fig-width: 10
#| fig-height: 12 

ggplot(simulation_parameters_long %>% filter(Measure=="Calibration Slope"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.2) +
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
    # scale_x_continuous(limits = c(-0.16, 0.18), breaks = seq(-.16, 0.17, by = 0.04)) +    
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick

```


### Bias 
The Bias was assessed for each simulation (where 0 indicates no bias and the model estimates are on average equal to the true values). 



``` {r fig-bias}
#| label: bias
#| echo: false
#| fig-cap: Bias for each combination of prevalence, sample size, missingness under Missing at Random
#| fig-width: 10
#| fig-height: 12 


 ggplot(simulation_parameters_long %>% filter(Measure=="Bias"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.2) +
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
   # scale_x_continuous(limits = c(0.042, 0.092), breaks = seq(0.04, 0.1, by = 0.004)) +    
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick
  
```
  
  

### Root Mean Square Error 
The RMSE was assessed for each simulation where lower error indicates a better fit of the model. 
The lowest prevalence simulations (1%) had the lowest mean square error. 

``` {r fig-rmse}
#| label: rmse
#| echo: false
#| fig-cap: RMSE for each combination of prevalence, sample size, missingness under Missing at Random
#| fig-width: 10
#| fig-height: 12 


  ggplot(simulation_parameters_long %>% filter(Measure=="Root Mean Square Error"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.2) +
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
    # xlim(0.72,0.79)+
  #  scale_x_continuous(limits = c(0.205, 0.305), breaks = seq(0.2, 0.30, by = 0.01)) +    
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick  
  
  
```

