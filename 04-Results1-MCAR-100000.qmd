---
title: "Results for MCAR N=100,000"
---

```{r}
#| label: load_packages 
#| include: false
library(dplyr)
library(ggplot2)
library(tidyr)
library(flextable)
```

```{r}
#| label: open_datasets_mcar_100000 
#| include: false
## Set working directory 
setwd("C:\\Users\\maecj\\OneDrive - Nexus365\\A DPhil\\Simulation studies\\Programs\\Study 1\\SimulationStudy1_11Jun2024\\SimulationStudy\\Data") 

## Load required datasets 
########################
load("MCAR_100000_Combined_03Oct2024.Rdata")
load("MCAR_100000_Combined_Long_03Oct2024.Rdata")
```

## Predictive Performance from Study 1 N=100,000 under Missing Completely at Random {#-predictive-performance-mar100000}

### Brier Score

There was no difference in performance of the imputation methods in any scenario. The lowest brier scores occurred at lower outcome prevalence.

```{r #fig-brier}
#| label: brier_score
#| echo: false
#| fig-cap: Brier Score for each combination of prevalence at n= 100,000 under Missing Completely at Random
#| fig-width: 10
#| fig-height: 12 

ggplot(simulation_parameters_long %>% filter(Measure=="Brier Score"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.8 ) +   
  geom_point(data = combined_df, aes(x = Brier, y = Method),              
             shape =4, position = position_jitter(width = 0.007), alpha = 0.5) + 
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
     scale_x_continuous(limits = c(0, 0.1), breaks = seq(0, 0.1, by = 0.025)) +    
    scale_colour_manual(values = c("Validation data, no missingness" = "grey",
                                   "Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    labs(caption = "The Brier score ranges between 0 (perfect accuracy) and 1 (perfect inaccuracy). This x scale ranges from 0 to 0.1") + 
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick
  
```

### Discrimination

The discrimination was highest for the CCA method under each scenario and averaged between 0.77 and 0.78 under each scenario. At all levels of missingness in the predictor, the multiple imputation methods had the lowest discrimination under each prevalence. The variation in the discrimination between methods was highest at 5% and 10% prevalence.

```{r #fig-auc}
#| label: auc
#| echo: false
#| fig-cap: AUC for each combination of prevalence at n= 100,000 under Missing Completely at Random
#| fig-width: 10
#| fig-height: 12 

ggplot(simulation_parameters_long %>% filter(Measure=="AUC"), 
         aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.8 ) +   
  geom_point(data = combined_df, aes(x = AUC, y = Method),              
             shape =4, position = position_jitter(width = 0.05), alpha = 0.5) + 
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)",
        caption = "The discrimination was calculated as the Area Under the Curve (AUC). Higher scores indicate better discrimination with 0.5 indicating the model is no better than chance.") +
    scale_x_continuous(limits = c(0.55, 1), breaks = seq(0.55, 1, by = 0.05)) + 
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick

```

### Calibration in the Large 

The Calibration was assessed through Calibration in the Large (CATL) and the Calibration Slope.

CCA and multiple imputation with or without the outcome were on average, closely calibrated with the non-missing scenario and close to zero, the multiple impuation methods has the largest range and confidence interval. Mean imputation calibration was on average higher than 0 (underestimating the risk) for most scenarios, particularly at high missingness (75%).

```{r #fig-calitl}
#| label: calitl
#| echo: false
#| fig-cap: Calibration in the Large for each combination of prevalence at n= 100,000 under Missing Completely at Random
#| fig-width: 10
#| fig-height: 12 

    ggplot(simulation_parameters_long %>% filter(Measure=="Calibration in the Large"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.8 ) +
      geom_point(data = combined_df, aes(x = Cal_Int, y = Method), 
             shape =4, position = position_jitter(width = 0.05), alpha = 0.5) + 
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    scale_colour_manual(values = c("Validation data, no missingness" = "grey",
                                    "Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
   labs(caption = "The ideal value of CATL is 0, which indicates perfect calibration, positive values indicate the model is underestimating the risk while negative values indicate overestimation. Larger deviations from 0 suggest poorer calibration.") +
      theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick
  
  
  
```

###  Calibration Slope

The multiple imputation methods had calibration less than 1 for all scenarios with the largest deviations at higher missingness (50% or 75%). The mean imputation and CCA had calibration avaerages most closes to the no missing scenario.

```{r #fig-calslope}
#| label: calslope
#| echo: false
#| fig-cap: Calibration Slope for each combination of prevalence at n=100,000 under Missing Completely at Random
#| fig-width: 10
#| fig-height: 12 

ggplot(simulation_parameters_long %>% filter(Measure=="Calibration Slope"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.8 ) +
  geom_point(data = combined_df, aes(x = Cal_Slope, y = Method), 
             shape =4, position = position_jitter(width = 0.05), alpha = 0.5) + 
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    scale_x_continuous(limits = c(0, 1.6), breaks = seq(0, 1.6, by = 0.5)) +    
    scale_colour_manual(values = c("Validation data, no missingness" = "grey",
                                   "Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    labs(caption = "The ideal value of the Calibration Slope is 1 indicating perfect calibration across all risk levels. Values less than 1 suggest overfitting (predictions are too extreme), while values greater than 1 suggest underfitting (predictions are too conservative). Values that differ significantly from 1 indicate poor calibration.") +
  theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick

```

### Bias

The Bias was assessed for each simulation (where 0 indicates no bias and the model estimates are on average equal to the true values).

```{r fig-bias}
#| label: bias
#| echo: false
#| fig-cap: Bias for each combination of prevalence at n= 100,000 under Missing Completely at Random
#| fig-width: 10
#| fig-height: 12 


 ggplot(simulation_parameters_long %>% filter(Measure=="Bias"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.8 ) +   
   geom_point(data = combined_df, aes(x = bias, y = Method),               
              shape =4, position = position_jitter(width = 0.05), alpha = 0.5) + 
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
   # scale_x_continuous(limits = c(0.042, 0.092), breaks = seq(0.04, 0.1, by = 0.004)) +    
    scale_colour_manual(values = c("Validation data, no missingness" = "grey",
                                   "Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick
  
```

### Root Mean Square Error

The RMSE was assessed for each simulation where lower error indicates a better fit of the model. The lowest prevalence simulations (1%) had the lowest mean square error.

```{r fig-rmse}
#| label: rmse
#| echo: false
#| fig-cap: RMSE for each combination of prevalence at n= 100,000 under Missing Completely at Random
#| fig-width: 10
#| fig-height: 12 


  ggplot(simulation_parameters_long %>% filter(Measure=="Root Mean Square Error"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.8 ) +   
    geom_point(data = combined_df, aes(x = rmse, y = Method),               
               shape =4, position = position_jitter(width = 0.05), alpha = 0.5) + 
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
    # xlim(0.72,0.79)+
  #  scale_x_continuous(limits = c(0.205, 0.305), breaks = seq(0.2, 0.30, by = 0.01)) +    
    scale_colour_manual(values = c("Validation data, no missingness" = "grey",
                                   "Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick  
  
  
```

Below, @tbl-results_mar100000 summarises the Performance Measures under N=100,000.

```{r, echo=FALSE}
#| label: tbl-results_mar100000
#| tbl-cap: Performance Measures at N=100,000 under MNAR

# table of results
  table_resultsn100000 <- simulation_parameters_long %>% 
        select("Parameter","Method", "Measure", "AVG", "LCI", "UCI", "NACount") %>%
        mutate(across(c(AVG, LCI, UCI),  ~ round(.x, 4)))
   
  ## quick table 
  table_resultsn100000 %>%
    knitr::kable(col.names = c("Scenario", "Method to handle missing data", "Performance Measure", "Average", "Lower Confidence Interval", "Upper Confidence Interval", "Number of simulations that failed to converge"))

```
