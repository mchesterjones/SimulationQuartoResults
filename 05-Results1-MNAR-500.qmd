---
title: "Results for Missing-Not-at-Random N=500"
---

## Results under Missing-Not-at-Random

In this scenario we include an unmeasured variable $U$ that induces a relationship between $R_1$ and $Y$. The Missing-Not-at-Random mechanism is described in the DAG below, (@fig-mnar).

As with the MAR scenario, this was repeated for sample sizes of $500$, $10,000$ and $100,000$ for development dataset and validation dataset ( $N_\text{dev}$ always equals $N_\text{val}$). The proportion of outcome varied between $1$% $5$% and $10$% and missingness in $X_1$ was varied between $25$%, $50$% and $75$%. The performance measures are compared below.

```{r flow chart caption}
#| label: fig-mnar
#| out-width: "491px"
#| fig-cap: "Missing-Not-at-Random DAG"
knitr::include_graphics("C:\\Users\\maecj\\OneDrive - Nexus365\\A DPhil\\Simulation studies\\Programs\\Study 1\\SimulationStudy1_11Jun2024\\QuartoResults\\figures\\MNAR.png")

```

```{r}
# Libraries 
#| label: load_packages 
#| echo: false
library(dplyr)
library(ggplot2)
library(tidyr)
library(flextable)


## Set working directory 
setwd("C:\\Users\\maecj\\OneDrive - Nexus365\\A DPhil\\Simulation studies\\Programs\\Study 1\\SimulationStudy1_11Jun2024\\SimulationStudy\\Data") 

# Data
load("MNAR_500_Combined_Long_30Aug2024.Rdata")

# Number of NAs
NAs_n500 <- simulation_parameters_long %>% filter(NACount!=0)


```

## Sample size N=500

At the smallest sample size, n=500. There were convergence issues where the outcome prevalence was lower (1% and 5%) and where Complete Case Analysis was used to handle the missing data as the sample size was reduced further. At missingness of 75%, more than half the simulations at 1% prevalence were unable to calculate the discrimination (AUC) (55%), calibration intercept (55%) and slope (57%), Table 4.

```{r}
# table of results
  table_resultsn500 <- simulation_parameters_long %>% 
        select(Parameter,"Method", "Measure", "AVG", "LCI", "UCI", "NACount") %>%
        mutate(across(c(AVG, LCI, UCI),  ~ round(.x, 4)))
   
  ## quick table 
  ft_results <- flextable(table_resultsn500) %>%
     set_caption(caption = "Table 4: Performance Measures at N=500 under MNAR")
  
```

#### Brier Score

The Brier score ranges between 0 (perfect accuracy) and 1 (perfect inaccuracy).

```{r #fig-brier}
#| label: brier_score
#| echo: false
#| fig-cap: Brier Score for each combination of prevalence, sample size, missingness under Missing at Random
#| fig-width: 10
#| fig-height: 12 

ggplot(simulation_parameters_long %>% filter(Measure=="Brier Score"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.2) +
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
    # scale_x_continuous(limits = c(0.042, 0.092), breaks = seq(0.04, 0.1, by = 0.004)) +    
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick
  
```

#### Discrimination

The discrimination was calculated as the Area Under the Curve (AUC). Higher scores indicate better discrimination with 0.5 indicating the model is no better than chance.

```{r #fig-auc}
#| label: auc
#| echo: false
#| fig-cap: AUC for each combination of prevalence, sample size, missingness under Missing at Random
#| fig-width: 10
#| fig-height: 12 

ggplot(simulation_parameters_long %>% filter(Measure=="AUC"), 
         aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.2) +
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)",
      caption = "Footnote: This graph shows the AUC for each combination of prevalence, sample size, and missingness under MNAR.") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
  #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
  #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
 #   xlim(0.72,0.79)+
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick

```

#### Calibration in the Large and Calibration Slope

The Calibration was assessed through Calibration in the Large (CATL) and the Calibration Slope.

The ideal value of CATL is 0, which indicates perfect calibration, positive values indicate the model is underestimating the risk while negative values indicate overestimation. Larger deviations from 0 suggest poorer calibration.

```{r #fig-calitl}
#| label: calitl
#| echo: false
#| fig-cap: Calibration in the Large for each combination of prevalence, sample size, missingness under Missing at Random
#| fig-width: 10
#| fig-height: 12 

    ggplot(simulation_parameters_long %>% filter(Measure=="Calibration in the Large"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.2) +
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick
  
  
```

The ideal value of the Calibration Slope is 1 indicating perfect calibration across all risk levels. Values less than 1 suggest overfitting (predictions are too extreme), while values greater than 1 suggest underfitting (predictions are too conservative). Values that differ significantly from 1 indicate poor calibration.

```{r #fig-calslope}
#| label: calslope
#| echo: false
#| fig-cap: Calibration Slope for each combination of prevalence, sample size, missingness under Missing at Random
#| fig-width: 10
#| fig-height: 12 

ggplot(simulation_parameters_long %>% filter(Measure=="Calibration Slope"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.2) +
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
    # scale_x_continuous(limits = c(-0.16, 0.18), breaks = seq(-.16, 0.17, by = 0.04)) +    
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick

```

### Bias

The Bias was assessed for each simulation (where 0 indicates no bias and the model estimates are on average equal to the true values).

```{r fig-bias}
#| label: bias
#| echo: false
#| fig-cap: Bias for each combination of prevalence, sample size, missingness under Missing at Random
#| fig-width: 10
#| fig-height: 12 


 ggplot(simulation_parameters_long %>% filter(Measure=="Bias"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.2) +
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
   # scale_x_continuous(limits = c(0.042, 0.092), breaks = seq(0.04, 0.1, by = 0.004)) +    
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick
  
```

### Root Mean Square Error

The RMSE was assessed for each simulation where lower error indicates a better fit of the model. The lowest prevalence simulations (1%) had the lowest mean square error.

```{r fig-rmse}
#| label: rmse
#| echo: false
#| fig-cap: RMSE for each combination of prevalence, sample size, missingness under Missing at Random
#| fig-width: 10
#| fig-height: 12 


  ggplot(simulation_parameters_long %>% filter(Measure=="Root Mean Square Error"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.2) +
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
    # xlim(0.72,0.79)+
  #  scale_x_continuous(limits = c(0.205, 0.305), breaks = seq(0.2, 0.30, by = 0.01)) +    
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick  
  
  
```
