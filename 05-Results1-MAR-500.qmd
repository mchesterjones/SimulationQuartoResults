---
title: "Results for MAR N=500"
---

```{r, echo=FALSE}
#| label: load_packages 
#| include: false
library(dplyr)
library(ggplot2)
library(tidyr)
library(flextable)
```

```{r , echo=FALSE}
#| label: open_datasets_mar_n500 
#| include: false
## Set working directory 
setwd("C:\\Users\\maecj\\OneDrive - Nexus365\\A DPhil\\Simulation studies\\Programs\\Study 1\\SimulationStudy1_11Jun2024\\SimulationStudy\\Data") 

## Load required datasets 
########################
load("MAR_500_Combined_Long_03Oct2024.Rdata")
load("MAR_500_Combined_03Oct2024.Rdata")
```

### Predictive Performance from Study 1 N=500 under Missing at Random {#-redictive-performance-n500}

At the smallest sample size, n=500. There were convergence issues where the outcome prevalence was lower (1% and 5%) and where Complete Case Analysis was used to handle the missing data as the sample size was reduced further. At missingness of 75%, almost half the simulations (46%) at 1% prevalence were unable to calculate the discrimination (AUC), calibration intercept and slope, @tbl-results_mar500.

#### Brier Score

At 75% missingness, the lowest average brier scores were observed for the CCA with the widest confidence intervals. There was limited differences in the brier scores between imputation methods, with overlapping confidence intervals. Lower prevalance scenarios had lower brier scores.

```{r #fig-brier}
#| label: brier_score
#| echo: false
#| fig-cap: Brier Score for each combination of prevalence at n=500 under Missing at Random
#| fig-width: 10
#| fig-height: 12 

ggplot(simulation_parameters_long %>% filter(Measure=="Brier Score"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.8 ) +   
  geom_point(data = combined_df, aes(x = Brier, y = Method),               
             shape =4, position = position_jitter(width = 0.0001), alpha = 0.5) + 
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
  
  ## Add X scale 
     scale_x_continuous(limits = c(0, 0.15), breaks = seq(0, 0.15, by = 0.025)) +     ## Add Colour Scale
    scale_colour_manual(values = c("Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    labs(caption = "The Brier score ranges between 0 (perfect accuracy) and 1 (perfect inaccuracy). This x scale runs from 0 to 0.15.") + 
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick
  
```

#### Discrimination

The confidence intervals overlapped between the imputation methods for each scenario.

```{r #fig-auc}
#| label: auc
#| echo: false
#| fig-cap: AUC for each combination of prevalence at n=500 under Missing at Random
#| fig-width: 10
#| fig-height: 12 


ggplot(simulation_parameters_long %>% filter(Measure=="AUC"), 
         aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.8 ) +   
  geom_point(data = combined_df, aes(x = AUC, y = Method),              
             shape =4, position = position_jitter(width = 0.05), alpha = 0.5) + 
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
       scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, by = 0.25)) + 
  ## Add Colour Scale 
  scale_colour_manual(values = c("Validation data, no missingness" = "grey",
                                   "Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
  labs(caption = "The discrimination was calculated as the Area Under the Curve (AUC). Higher scores indicate better discrimination with 0.5 indicating the model is no better than chance.") +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick

```

#### Calibration in the Large and Calibration Slope

The Calibration was assessed through Calibration in the Large (CATL) and the Calibration Slope.

The ideal value of CATL is 0, which indicates perfect calibration, positive values indicate the model is underestimating the risk while negative values indicate overestimation. Larger deviations from 0 suggest poorer calibration.

```{r #fig-calitl}
#| label: calitl
#| echo: false
#| fig-cap: Calibration in the Large for each combination of prevalence at n=500 under Missing at Random
#| fig-width: 10
#| fig-height: 12 

    ggplot(simulation_parameters_long %>% filter(Measure=="Calibration in the Large"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.8 ) +   
      geom_point(data = combined_df, aes(x = Cal_Int, y = Method),              
                 shape =4, position = position_jitter(width = 0.001), alpha = 0.5) + 
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    scale_colour_manual(values = c("Validation data, no missingness" = "grey",
                                   "Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick
  
  
```

The ideal value of the Calibration Slope is 1 indicating perfect calibration across all risk levels. Values less than 1 suggest overfitting (predictions are too extreme), while values greater than 1 suggest underfitting (predictions are too conservative). Values that differ significantly from 1 indicate poor calibration.

```{r #fig-calslope}
#| label: calslope
#| echo: false
#| fig-cap: Calibration Slope for each combination of prevalence at n=500 under Missing at Random
#| fig-width: 10
#| fig-height: 12 

ggplot(simulation_parameters_long %>% filter(Measure=="Calibration Slope"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.8 ) +   
  geom_point(data = combined_df, aes(x = Cal_Slope, y = Method),              
             shape =4, position = position_jitter(width = 0.05), alpha = 0.5) + 
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
    # scale_x_continuous(limits = c(-0.16, 0.18), breaks = seq(-.16, 0.17, by = 0.04)) +    
    scale_colour_manual(values = c("Validation data, no missingness" = "grey",
                                   "Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick

```

### Bias

The Bias was assessed for each simulation (where 0 indicates no bias and the model estimates are on average equal to the true values).

```{r fig-bias}
#| label: bias
#| echo: false
#| fig-cap: Bias for each combination of prevalence at n=500 under Missing at Random
#| fig-width: 10
#| fig-height: 12 


 ggplot(simulation_parameters_long %>% filter(Measure=="Bias"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.8 ) +  
   geom_point(data = combined_df, aes(x = bias, y = Method),              
              shape =4, position = position_jitter(width = 0.05), alpha = 0.5) + 
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
   # scale_x_continuous(limits = c(0.042, 0.092), breaks = seq(0.04, 0.1, by = 0.004)) +    
    scale_colour_manual(values = c("Validation data, no missingness" = "grey",
                                   "Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick
  
```

### Root Mean Square Error

The RMSE was assessed for each simulation where lower error indicates a better fit of the model. The lowest prevalence simulations (1%) had the lowest mean square error.

```{r fig-rmse}
#| label: rmse
#| echo: false
#| fig-cap: RMSE for each combination of prevalence at n=500 under Missing at Random
#| fig-width: 10
#| fig-height: 12 


  ggplot(simulation_parameters_long %>% filter(Measure=="Root Mean Square Error"), 
           aes(x = AVG, y = Method, colour = Method)) +
    geom_point(size = 3) +
    geom_errorbar(aes(xmin = LCI, xmax = UCI), width = 0.8 ) +   
    geom_point(data = combined_df, aes(x = rmse, y = Method),               
               shape =4, position = position_jitter(width = 0.001), alpha = 0.5) + 
    labs(y = NULL,
         x = NULL,
         colour = "Method\n(Mean, 95% CI)") +
    theme_minimal() + 
    facet_wrap( ~Parameter , scales = "fixed", ncol=1) + 
    #  facet_grid( Parameter ~ Measure, scales = "fixed") + 
    #  facet_wrap(Measure ~ Parameter, scales = "free_x") + 
    # xlim(0.72,0.79)+
  #  scale_x_continuous(limits = c(0.205, 0.305), breaks = seq(0.2, 0.30, by = 0.01)) +    
    scale_colour_manual(values = c("Validation data, no missingness" = "grey",
                                   "Complete Case Analysis" = "blue", 
                                   "Mean Imputation" = "red", 
                                   "Multiple Imputation with Outcome" = "green",
                                   "Multiple Imputation without Outcome" = "purple")) +
       labs(caption = "Bias of 0 indicates no bias and the model estimates are on average equal to the true values).") +
    theme(legend.position = "right",
          strip.text = element_text(size = 14),  # Customize strip text size
          strip.placement = "outside",  # Place strip labels outside the plot area
          strip.background = element_blank(),  # Remove strip background
          axis.title.x = element_text(size = 14), 
          axis.title.y = element_text(size = 14), 
          axis.text.x = element_text(size = 12), 
          axis.text.y = element_blank(),  # Remove y-axis text
          axis.ticks.y = element_blank())  # Remove y-axis tick  
  
  
```

The average performance measures across 100 simulations are summarised in @tbl-results_mar500.

```{r, echo=FALSE}
#| label: tbl-results_mar500
#| tbl-cap: Performance Measures at N=500 under MAR

# table of results
  table_resultsn500 <- simulation_parameters_long %>% 
        select("Parameter","Method", "Measure", "AVG", "LCI", "UCI", "NACount") %>%
        mutate(across(c(AVG, LCI, UCI),  ~ round(.x, 4)))
   
  ## quick table 
  table_resultsn500 %>%
    knitr::kable(col.names = c("Scenario", "Method to handle missing data", "Performance Measure", "Average", "Lower Confidence Interval", "Upper Confidence Interval", "Number of simulations that failed to converge"))
                               
                               

  
```
